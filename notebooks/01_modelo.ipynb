{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6284be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2935781",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW = Path(\"../data/raw\")\n",
    "PROC = Path(\"../data/processed\")\n",
    "PROC.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb395355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta base: /Users/sonsifabini\n",
      "Existe RAW? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta base del proyecto (sube 3 niveles desde notebooks/data/processed/)\n",
    "BASE = Path(__file__).resolve().parents[3] if \"__file__\" in locals() else Path().resolve().parents[2]\n",
    "\n",
    "RAW = Path(\"../data/raw\")\n",
    "PROC = BASE / \"data\" / \"processed\"\n",
    "\n",
    "print(\"Ruta base:\", BASE)\n",
    "print(\"Existe RAW?\", RAW.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81dc9e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q1/tzbwph8d4qlfxl59ygs8f3dm0000gn/T/ipykernel_66228/822383569.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  orders = pd.read_csv(\n",
      "/var/folders/q1/tzbwph8d4qlfxl59ygs8f3dm0000gn/T/ipykernel_66228/822383569.py:7: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  clients = pd.read_csv(\n",
      "/var/folders/q1/tzbwph8d4qlfxl59ygs8f3dm0000gn/T/ipykernel_66228/822383569.py:12: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  products = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# === 1) Cargar datos con los nombres reales ===\n",
    "orders = pd.read_csv(\n",
    "    RAW / \"orders_202510280929.csv\",\n",
    "    parse_dates=[\"order_timestamp\"],\n",
    "    infer_datetime_format=True\n",
    ")\n",
    "clients = pd.read_csv(\n",
    "    RAW / \"clients_202510280926.csv\",\n",
    "    parse_dates=[\"registration_date\", \"last_seen\"],\n",
    "    infer_datetime_format=True\n",
    ")\n",
    "products = pd.read_csv(\n",
    "    RAW / \"products_202510280931.csv\",\n",
    "    parse_dates=[\"last_updated\", \"created_at\"],\n",
    "    infer_datetime_format=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e6361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos útiles\n",
    "orders[\"order_price\"] = orders[\"order_price\"].astype(float)\n",
    "orders[\"shipping_cost\"] = orders[\"shipping_cost\"].astype(float)\n",
    "\n",
    "# === 2) Definir ventana temporal ===\n",
    "# T0 como última fecha observada en órdenes válidas\n",
    "# (ajustá statuses válidos según tus datos reales)\n",
    "VALID_STATUS = {\"delivered\", \"completed\", \"shipped\"}   # ejemplo\n",
    "orders_valid = orders[orders[\"order_status\"].str.lower().isin(VALID_STATUS)].copy()\n",
    "\n",
    "T0 = orders_valid[\"order_timestamp\"].max().normalize()\n",
    "L_days = 180\n",
    "P_days = 90\n",
    "L = pd.Timedelta(days=L_days)\n",
    "P = pd.Timedelta(days=P_days)\n",
    "\n",
    "obs_start  = T0 - L\n",
    "pred_start = T0\n",
    "pred_end   = T0 + P\n",
    "\n",
    "# Particiones para features/label\n",
    "orders_feat = orders_valid[(orders_valid[\"order_timestamp\"] >= obs_start) &\n",
    "                           (orders_valid[\"order_timestamp\"] < T0)].copy()\n",
    "orders_pred = orders_valid[(orders_valid[\"order_timestamp\"] >= pred_start) &\n",
    "                           (orders_valid[\"order_timestamp\"] < pred_end)].copy()\n",
    "\n",
    "# === 3) Enriquecer órdenes con info de producto (opcional)\n",
    "orders_feat = orders_feat.merge(\n",
    "    products[[\"product_id\",\"category\",\"brand\"]],\n",
    "    on=\"product_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def997a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q1/tzbwph8d4qlfxl59ygs8f3dm0000gn/T/ipykernel_66228/1848591849.py:58: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cad = of.groupby(\"client_id\").apply(cadence_days).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# === 4) Construcción de features por cliente (RFM + comportamiento) ===\n",
    "def build_features(orders_feat: pd.DataFrame, clients: pd.DataFrame) -> pd.DataFrame:\n",
    "    base = clients[[\"client_id\",\"registration_date\",\"last_seen\"]].copy()\n",
    "\n",
    "    if orders_feat.empty:\n",
    "        # Sin compras en la ventana: rellenamos defaults + recency desde last_seen\n",
    "        feat = base.copy()\n",
    "        feat[\"recency_days\"]      = (T0 - feat[\"last_seen\"]).dt.days\n",
    "        feat[\"freq_orders\"]       = 0\n",
    "        feat[\"monetary_total\"]    = 0.0\n",
    "        feat[\"monetary_avg\"]      = 0.0\n",
    "        feat[\"monetary_max\"]      = 0.0\n",
    "        feat[\"tenure_days\"]       = (T0 - feat[\"registration_date\"]).dt.days\n",
    "        feat[\"uniq_products\"]     = 0\n",
    "        feat[\"uniq_categories\"]   = 0\n",
    "        feat[\"uniq_brands\"]       = 0\n",
    "        feat[\"cadence_avg_days\"]  = np.nan\n",
    "        feat[\"orders_last30\"]     = 0\n",
    "        feat[\"amt_last30\"]        = 0.0\n",
    "        feat[\"pct_express\"]       = 0.0\n",
    "        feat[\"pct_standard\"]      = 0.0\n",
    "        feat[\"avg_shipping_cost\"] = 0.0\n",
    "        feat[\"pct_card\"]          = 0.0\n",
    "        feat[\"pct_cash\"]          = 0.0\n",
    "        feat[\"pct_wallet\"]        = 0.0\n",
    "        return feat\n",
    "\n",
    "    of = orders_feat.copy()\n",
    "    of = of.sort_values([\"client_id\",\"order_timestamp\"])\n",
    "\n",
    "    g = of.groupby(\"client_id\", as_index=False)\n",
    "    agg = g.agg(\n",
    "        freq_orders     = (\"order_id\",\"count\"),\n",
    "        monetary_total  = (\"order_price\",\"sum\"),\n",
    "        monetary_avg    = (\"order_price\",\"mean\"),\n",
    "        monetary_max    = (\"order_price\",\"max\"),\n",
    "        last_order_ts   = (\"order_timestamp\",\"max\"),\n",
    "        first_order_ts  = (\"order_timestamp\",\"min\"),\n",
    "        avg_shipping_cost = (\"shipping_cost\",\"mean\")\n",
    "    )\n",
    "\n",
    "    # Recency y Tenure\n",
    "    agg[\"recency_days\"] = (T0 - agg[\"last_order_ts\"]).dt.days\n",
    "    agg = agg.merge(base[[\"client_id\",\"registration_date\"]], on=\"client_id\", how=\"left\")\n",
    "    agg[\"tenure_days\"]  = (T0 - agg[\"registration_date\"]).dt.days\n",
    "\n",
    "    # Variedad\n",
    "    uniq_products   = of.groupby(\"client_id\")[\"product_id\"].nunique().rename(\"uniq_products\")\n",
    "    uniq_categories = of.groupby(\"client_id\")[\"category\"].nunique().rename(\"uniq_categories\")\n",
    "    uniq_brands     = of.groupby(\"client_id\")[\"brand\"].nunique().rename(\"uniq_brands\")\n",
    "    for s in (uniq_products, uniq_categories, uniq_brands):\n",
    "        agg = agg.merge(s, on=\"client_id\", how=\"left\")\n",
    "\n",
    "    # Cadencia promedio\n",
    "    def cadence_days(gdf):\n",
    "        d = gdf[\"order_timestamp\"].sort_values().diff().dt.days.dropna()\n",
    "        return pd.Series({\"cadence_avg_days\": d.mean() if len(d)>0 else np.nan})\n",
    "    cad = of.groupby(\"client_id\").apply(cadence_days).reset_index()\n",
    "    agg = agg.merge(cad, on=\"client_id\", how=\"left\")\n",
    "\n",
    "    # Últimos 30 días\n",
    "    last30 = of[of[\"order_timestamp\"] >= (T0 - pd.Timedelta(days=30))]\n",
    "    last30_agg = last30.groupby(\"client_id\").agg(\n",
    "        orders_last30=(\"order_id\",\"count\"),\n",
    "        amt_last30=(\"order_price\",\"sum\")\n",
    "    ).reset_index()\n",
    "    agg = agg.merge(last30_agg, on=\"client_id\", how=\"left\").fillna({\"orders_last30\":0,\"amt_last30\":0})\n",
    "\n",
    "    # Mix de shipping\n",
    "    ship_pivot = (\n",
    "        of.pivot_table(index=\"client_id\", columns=\"shipping_method\", values=\"order_id\",\n",
    "                       aggfunc=\"count\", fill_value=0)\n",
    "        .add_prefix(\"ship_\").reset_index()\n",
    "    )\n",
    "    agg = agg.merge(ship_pivot, on=\"client_id\", how=\"left\")\n",
    "    ship_cols = [c for c in agg.columns if c.startswith(\"ship_\")]\n",
    "    agg[\"ship_total\"] = agg[ship_cols].sum(axis=1).replace(0, np.nan)\n",
    "    agg[\"pct_express\"]  = (agg.get(\"ship_express\",0)  / agg[\"ship_total\"]).fillna(0)\n",
    "    agg[\"pct_standard\"] = (agg.get(\"ship_standard\",0) / agg[\"ship_total\"]).fillna(0)\n",
    "    agg = agg.drop(columns=ship_cols + [\"ship_total\"], errors=\"ignore\")\n",
    "\n",
    "    # Mix de payment_method (proporciones)\n",
    "    pay_pivot = (\n",
    "        of.pivot_table(index=\"client_id\", columns=\"payment_method\", values=\"order_id\",\n",
    "                       aggfunc=\"count\", fill_value=0)\n",
    "        .add_prefix(\"pay_\").reset_index()\n",
    "    )\n",
    "    agg = agg.merge(pay_pivot, on=\"client_id\", how=\"left\")\n",
    "    pay_cols = [c for c in agg.columns if c.startswith(\"pay_\")]\n",
    "    if pay_cols:\n",
    "        agg[\"pay_total\"] = agg[pay_cols].sum(axis=1).replace(0, np.nan)\n",
    "        # mapeo simple a 3 buckets comunes; ajustá si tenés otros\n",
    "        agg[\"pct_card\"]   = (agg[[c for c in pay_cols if \"card\"   in c]].sum(axis=1) / agg[\"pay_total\"]).fillna(0)\n",
    "        agg[\"pct_cash\"]   = (agg[[c for c in pay_cols if \"cash\"   in c]].sum(axis=1) / agg[\"pay_total\"]).fillna(0)\n",
    "        agg[\"pct_wallet\"] = (agg[[c for c in pay_cols if \"wallet\" in c]].sum(axis=1) / agg[\"pay_total\"]).fillna(0)\n",
    "        agg = agg.drop(columns=pay_cols + [\"pay_total\"], errors=\"ignore\")\n",
    "    else:\n",
    "        agg[\"pct_card\"] = agg[\"pct_cash\"] = agg[\"pct_wallet\"] = 0.0\n",
    "\n",
    "    # Completar con clientes sin compras en L\n",
    "    feat = base.merge(agg.drop(columns=[\"registration_date\"], errors=\"ignore\"),\n",
    "                      on=\"client_id\", how=\"left\")\n",
    "    fill_zeros = [\"freq_orders\",\"monetary_total\",\"monetary_avg\",\"monetary_max\",\n",
    "                  \"uniq_products\",\"uniq_categories\",\"uniq_brands\",\n",
    "                  \"orders_last30\",\"amt_last30\",\"pct_express\",\"pct_standard\",\n",
    "                  \"avg_shipping_cost\",\"pct_card\",\"pct_cash\",\"pct_wallet\"]\n",
    "    for c in fill_zeros:\n",
    "        feat[c] = feat[c].fillna(0 if c!=\"avg_shipping_cost\" else 0.0)\n",
    "    feat[\"recency_days\"] = feat[\"recency_days\"].fillna((T0 - feat[\"last_seen\"]).dt.days)\n",
    "    feat[\"tenure_days\"]  = feat[\"tenure_days\"].fillna((T0 - feat[\"registration_date\"]).dt.days)\n",
    "    return feat.drop(columns=[\"last_seen\"])\n",
    "\n",
    "features = build_features(orders_feat, clients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
